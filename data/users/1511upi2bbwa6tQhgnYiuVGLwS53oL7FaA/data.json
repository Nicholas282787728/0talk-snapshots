{
	"next_topic_id": 1,
	"topic": [],
	"topic_vote": {},
	"next_comment_id": 5,
	"comment": {
		"1_1BG8WrV2VY1bNjQiKR4tcL52FAw6Y6BoJ8": [
			{
				"comment_id": 1,
				"body": "一款数字产品的生命力，取决于有多少人愿意使用它。从目前全球大量互联网用户尤其是中文用户涌入社区的情况来看，zeronet前景光明。\n在80年代以前，互联网原本就是分布式的存在。现在的互联网不过是一个被ISP和商业巨头控制的监控网络，许多有知觉的用户早已认识到这种危险的境况，因而开发和支持新的技术来重建分布式的网络。BT、emule直到今天zeronet，自由精神多年来一直在网络世界里顽强生长、蓬勃壮大。\n我个人认为zeronet是一个划时代的产品，它通过分布式技术使网络内容的存储成本降为0，使信息无法被审查和删除，使用户摆脱对“中心”的依赖建立自己的网络社区，开源共享和言论自由的精神得以延续。\n在zeronet里，没有任何信息或人是重要或不重要的，任何个体都是平等的，“长尾效应”将在这里得到最充分的体现，无数人类的思想将在这里汇集、交融，迸发出创新的火花。\n感谢洋葱路由，zeronet的分布式网络在保持便利性的同时也能保证用户的匿名和安全。过去，能同时实现便利和匿名的产品不多，zeronet做到了，并受到了公众的认可，它的成功不可避免。",
				"added": 1468289663
			},
			{
				"comment_id": 3,
				"body": "> [frog](#comment_4_1D9PFKqnik1Z4JtDZnYhyQERZAu5ZiXqFX): zeronet的p2p不止是体现在存储上，更是信息的流动有别于传统web。其实对于存储我认为并不是大问题，只要每个的site owner保证完整存储自己site的数据，就可以了，其他peer都只是作为一种作为加速下载的辅助，（确实可能需要几个稳定的中继节点来保证所有数据都是可获取的）普通用户存储的数据是volatile的，只需要存储自己关心的数据即可。 我认为真正问题所在是scalability比较差，用户不能精确地选择自己需要的数据，我也不清楚zeronet的具体运作模式，不过在使用过程中我感觉，它是以site为单位，完整下载下来的，这点确实是个问题，不过基于zeronet的p2p的性质，我想可能有如下几点原因：1) 增大用户下载的数据量，有助于加快下载速度，即数据冗余与下载速度是反比关系。2) 提前缓存更多数据，有助于降低浏览时的延迟，各位应该有所体会，zeronet在缓存完毕后浏览时的速度是很快的，而post时的延迟也是不小的，实为p2p的特性所致，现在的用户数不多，所以数据同步很难做到实时，唯有通过缓存来提高浏览体验。如果可以实现传统web中那样请求时下载，则不会出现题主所说“存储成本过大”的问题，但延迟的问题会暴露出来。我想在用户数达到一定量级后，上面的问题可以得到缓解。现阶段，我认为可以采取以下措施：1）增加几个稳定的中继节点，用来加速。 （有被墙的风险）2）细化用户下载的数据的情景，适当减少数据冗余。\n\n个人认为无需中继节点，一旦有中继，就是伪去中心化网络了。\n1、中继的成本很难有人和组织来负担。\n2、去中心化的网路以关注度作为带宽，体现了民主价值。好的网站关注的人多，自然带宽大速度快，不好的网站关注的人少，带宽小速度慢，可起到优胜劣汰的作用。",
				"added": 1468313814
			},
			{
				"comment_id": 4,
				"body": "而且我个人认为数据的冗余度越大，网络就越是健壮和安全。反正用户的硬盘没有成本，何必在乎数据的重复呢？",
				"added": 1468313986
			}
		],
		"2_1J3rJ8ecnwH2EPYa6MrgZttBNc61ACFiCj": [
			{
				"comment_id": 2,
				"body": "Frozen!",
				"added": 1468304156
			}
		]
	},
	"comment_vote": {}
}